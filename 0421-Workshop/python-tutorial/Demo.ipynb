{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Libraries\n",
    "import uproot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Local stuff\n",
    "from branches import *\n",
    "from panda_helpers import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the file you want to open\n",
    "\n",
    "# Make sure it is a __flat__ CAF file!!!\n",
    "filename = \"/pnfs/sbnd/persistent/sbndpro/mcp/mc/workshop/SBNWorkshop0421/prodoverlay_corsika_cosmics_proton_genie_nu_spill_gsimple-configf-v1_tpc/v09_19_00_01/caf/flat_caf_0-9f00feff-e742-419d-9856-9fe7428b93a9.root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open it in uproot\n",
    "\n",
    "tname = \"recTree\"\n",
    "events = uproot.open(filename + \":\" + tname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a TTree!\n",
    "\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take some branches and load them into a pandas \"data frame\"\n",
    "\n",
    "# What is a data frame?\n",
    "# It is the way that pandas represents data. It works a lot like an excel spreedsheet or a SQL database.\n",
    "# Each row of the data frame is a thing. That thing can have a number of different values, which are represented\n",
    "# by columns.\n",
    "\n",
    "# So, for example. We can make a neutrino data frame. Each row of the data frame is a neutrino. A neutrino\n",
    "# has a bunch of different values like its Energy, interaction vertex, etc.\n",
    "\n",
    "# mcbranches: a bunch of branch names that provide information on the neutrino interaction \n",
    "nudf = events.arrays(mcbranches, library=\"pd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data frame!!\n",
    "\n",
    "# \"entry\" corresponds to the spill. And \"subentry\" corresponds to the index of the neutr\n",
    "nudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot!\n",
    "_ = plt.hist(nudf[\"rec.mc.nu.E\"])\n",
    "plt.xlabel(\"Neutrino Energy [GeV]\")\n",
    "plt.ylabel(\"Entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we access more complex data?\n",
    "\n",
    "# The CAF files have information we need to do event selection type analysis with. That is a lot of information. \n",
    "# Stuff like:\n",
    "# -True neutrino interactions\n",
    "# -True G4 particles\n",
    "# -Reconstructed tracks\n",
    "# -Reconstructed showers\n",
    "# -CRT matching\n",
    "# -PMT matching\n",
    "# -Reco-to-Truth matching\n",
    "# -...and more\n",
    "\n",
    "# How do we handle this data in pandas, where every row is a thing?\n",
    "\n",
    "# Two complexities:\n",
    "# 1. We cannot combine (for example) reconstructed tracks and true neutrinos in the same data frame. Every row can\n",
    "#    be a track, or every row can be a neutrino. But every row cannot be a track/neutrino thing.\n",
    "#\n",
    "#    So we are going to start out with separate data frame for each thing, and then build them together into a\n",
    "#    single data frame that has all the information we need.\n",
    "#\n",
    "# 2. We need to handle nested data. A single spill might have multiple neutrinos. Or a single spill might have\n",
    "#    multiple reconstructed slices, which have multiple reconstructed tracks, which have multiple truth-matched\n",
    "#    paricles, etc.\n",
    "# \n",
    "#    In C++, this is represented as a bunch of nested vectors. In pandas, we can use a __MultiIndex__. This is \n",
    "#    a very powerful pandas construct that lets us represent nested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to handle nested data, I have a helper function that parses the FLAT CAF structure\n",
    "\n",
    "# delete the old nudf\n",
    "del nudf\n",
    "nudf = loadbranches(events, mcbranches)\n",
    "slcdf = loadbranches(events, slcbranches)\n",
    "trkdf = loadbranches(events, trkbranches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neutrino data frame!\n",
    "nudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The slice data frame!\n",
    "slcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The track data frame!\n",
    "trkdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we are going to a quick bit of the numu Event selection.\n",
    "\n",
    "# First, we can look at some reconstructed variables and compare them for neutrino-slices and cosmic-slices.\n",
    "# To do this, we need to merge the neutrino data frame into the slice data frame using truth matching.\n",
    "\n",
    "# We can do this with a \"merge\"\n",
    "\n",
    "# Cut on the truth matching -- require the slice contains more than half of the deposited neutrino energy.\n",
    "# This ensures that each neutrino can only have one reconstructed slice match\n",
    "slc_has_nu_match = slcdf[\"rec.slc.tmatch.eff\"] > 0.5\n",
    "\n",
    "# Ignore index matches where the efficiency cut fails\n",
    "slcdf.loc[np.invert(slc_has_nu_match) & (slcdf[\"rec.slc.tmatch.index\"] >= 0), \"rec.slc.tmatch.index\"] = np.nan\n",
    "\n",
    "matchdf = pd.merge(slcdf.reset_index(), # Merging can mess with the multi-index -- we'll fix this later\n",
    "                 nudf.reset_index(),\n",
    "                 left_on=[\"entry\", \"rec.slc.tmatch.index\"], # Match on the entry number than the neutrino index\n",
    "                 right_on=[\"entry\", \"rec.mc.nu..index\"], \n",
    "                 how=\"left\", # Keep every slice\n",
    "                 )\n",
    "\n",
    "matchdf = matchdf.set_index([\"entry\", \"rec.slc..index\"], verify_integrity=True)\n",
    "\n",
    "# check that each neutrino matches to only one slice\n",
    "assert(matchdf.groupby([\"entry\", \"rec.mc.nu..index\"])[\"rec.slc.charge\"].count().max() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy of neutrinos with a matched slice!!\n",
    "\n",
    "# NOTE: one thing to keep in mind -- merge's make a cut on the physics\n",
    "\n",
    "_ = plt.hist(matchdf[\"rec.mc.nu.E\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can look at an example slice variable -- the Pandora \"nu\" score\n",
    "\n",
    "var = matchdf[\"rec.slc.nu_score\"]\n",
    "\n",
    "is_numu_cc = (np.abs(matchdf[\"rec.mc.nu.pdg\"]) == 14) & (matchdf[\"rec.mc.nu.iscc\"])\n",
    "is_cosmic = (matchdf[\"rec.slc.tmatch.index\"] < 0)\n",
    "bins=np.linspace(0, 1, 21)\n",
    "_ = plt.hist(var[is_numu_cc], bins=bins, histtype=\"step\", label=r\"$\\nu_\\mu$ CC\")\n",
    "_ = plt.hist(var[is_cosmic], bins=bins, histtype=\"step\", label=\"Cosmic\")\n",
    "_ = plt.legend()\n",
    "_ = plt.xlabel(r\"Pandora $\\nu$ Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complicated event selection stuff -- select a primary \"Muon\" track\n",
    "\n",
    "# Using the algorithm from:\n",
    "# https://sbn-docdb.fnal.gov/cgi-bin/private/RetrieveFile?docid=20139&filename=event_selection.pdf&version=2\n",
    "\n",
    "# Now, using the trkdf\n",
    "trkdf[\"trk_contained\"] =\\\n",
    "    InFV(trkdf[\"rec.slc.reco.trk.start.x\"], trkdf[\"rec.slc.reco.trk.start.y\"], trkdf[\"rec.slc.reco.trk.start.z\"]) &\\\n",
    "    InFV(trkdf[\"rec.slc.reco.trk.end.x\"], trkdf[\"rec.slc.reco.trk.end.y\"], trkdf[\"rec.slc.reco.trk.end.z\"])\n",
    "\n",
    "# check valid chi2 -- just look at collection plane for now\n",
    "muon_chi2 = (trkdf[\"rec.slc.reco.trk.chi2pid2.chi2_muon\"] < 30.) &\\\n",
    "    (trkdf[\"rec.slc.reco.trk.chi2pid2.chi2_proton\"] > 60.)\n",
    "\n",
    "# Valid primary track candidates\n",
    "primary_track_candidate = (trkdf[\"trk_contained\"] & muon_chi2 & (trkdf[\"rec.slc.reco.trk.len\"] > 50.)) |\\\n",
    "        (trkdf[\"rec.slc.reco.trk.len\"] > 100.)\n",
    "\n",
    "\n",
    "primary_track = trkdf[primary_track_candidate]\\\n",
    "    .sort_values([\"entry\", \"rec.slc..index\", 'rec.slc.reco.trk.len'], ascending=[True, True, False])\\\n",
    "    .groupby([\"entry\", \"rec.slc..index\"]).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, merge the primary track into the slice df\n",
    "df = pd.merge(matchdf.reset_index(), primary_track,\n",
    "              left_on=[\"entry\", \"rec.slc..index\"], # match on spill then slice number\n",
    "              right_on=[\"entry\", \"rec.slc..index\"],\n",
    "              how=\"inner\", # only keep slices with a primary track\n",
    "              validate=\"one_to_one\", # Always validate when you can! Don't put two primary tracks in a slice -- this would double-count a slice\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can compute more stuff! Like the primary track momentum\n",
    "\n",
    "is_numu_cc = (np.abs(df[\"rec.mc.nu.pdg\"]) == 14) & (df[\"rec.mc.nu.iscc\"])\n",
    "is_cosmic = (df[\"rec.slc.tmatch.index\"] < 0)\n",
    "\n",
    "# Muon range momentum for contained tracks\n",
    "var = df[\"rec.slc.reco.trk.rangeP.p_muon\"]\n",
    "bins = np.linspace(0, 2, 21)\n",
    "_ = plt.hist(var[is_numu_cc & df[\"trk_contained\"]], bins=bins, histtype=\"step\", label=r\"$\\nu_\\mu$ CC\")\n",
    "_ = plt.hist(var[is_cosmic & df[\"trk_contained\"]], bins=bins, histtype=\"step\", label=\"Cosmic\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Reconstructed Muon Momentum (Contained) [GeV/c]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And MCS for exiting tracks\n",
    "\n",
    "var = df[\"rec.slc.reco.trk.mcsP.fwdP_muon\"]\n",
    "bins = np.linspace(0, 2, 21)\n",
    "_ = plt.hist(var[is_numu_cc & np.invert(df[\"trk_contained\"])], bins=bins, histtype=\"step\", label=r\"$\\nu_\\mu$ CC\")\n",
    "_ = plt.hist(var[is_cosmic & np.invert(df[\"trk_contained\"])], bins=bins, histtype=\"step\", label=\"Cosmic\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Reconstructed Muon Momentum (Exiting) [GeV/c]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two\n",
    "recop = df[\"rec.slc.reco.trk.rangeP.p_muon\"]+0. # copy\n",
    "recop[np.invert(df[\"trk_contained\"])] = df.loc[np.invert(df[\"trk_contained\"]), \"rec.slc.reco.trk.mcsP.fwdP_muon\"]\n",
    "\n",
    "\n",
    "var = recop\n",
    "_ = plt.hist(var[is_numu_cc], bins=bins, histtype=\"step\", label=r\"$\\nu_\\mu$ CC\")\n",
    "_ = plt.hist(var[is_cosmic], bins=bins, histtype=\"step\", label=\"Cosmic\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Reconstructed Muon Momentum [GeV/c]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some cuts!\n",
    "passcut = (df[\"rec.slc.nu_score\"] > 0.5) # Good nu score -- also rejects Clear-Cosmics (which have -1)\n",
    "\n",
    "_ = plt.hist(var[is_numu_cc & passcut], bins=bins, histtype=\"step\", label=r\"$\\nu_\\mu$ CC\")\n",
    "_ = plt.hist(var[is_cosmic & passcut], bins=bins, histtype=\"step\", label=\"Cosmic\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Reconstructed Muon Momentum [GeV/c]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-volleyball",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
